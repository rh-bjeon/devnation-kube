
= Kubernetes를 통한 애플리케이션 고가용성

== 고가용성 애플리케이션 배포 개념
고가용성 (HA)은 애플리케이션을 더욱 강력하게 만들고 런타임 오류를 방지하는 것을 목표로 합니다. HA 기술을 구현하면 사용자가 애플리케이션을 완전히 사용할 수 없게 될 가능성이 줄어듭니다.

일반적으로 HA는 다음과 같은 상황에서 애플리케이션을 오류로부터 보호할 수 있습니다.

* 애플리케이션 버그 형태의 그 자체에서

* 네트워킹 문제와 같은 환경에서

* 클러스터 리소스를 소모하는 다른 애플리케이션에서

또한 HA 사례는 메모리 누수가 있는 애플리케이션과 같은 애플리케이션으로부터 클러스터를 보호할 수 있습니다.

== 신뢰성 있는 애플리케이션 작성
기본적으로 클러스터 수준 HA 툴링은 최악의 시나리오를 완화합니다. HA는 애플리케이션 수준 문제를 해결하는 용도로는 사용할 수 없지만 개발자의 완화 기능을 강화합니다. 애플리케이션 보안은 안정성을 위해 필요하지만 별개의 문제입니다.

Kubernetes가 장애 시나리오를 가장 잘 처리할 수 있도록 애플리케이션이 클러스터와 함께 작동해야 합니다. Kubernetes는 애플리케이션에서 다음 동작을 예상합니다.

* 재시작 허용

* 시작, 준비 상태, 활성 프로브 등의 상태 프로브에 응답

* 여러 개의 동시 인스턴스 지원

* 잘 정의되고 올바르게 작동하는 리소스 사용

* 제한된 권한으로 작동

클러스터는 앞의 동작이 없는 애플리케이션을 실행할 수 있지만 이러한 동작이 있는 애플리케이션은 Kubernetes가 제공하는 안정성 및 HA 기능을 더 잘 사용합니다.

대부분의 HTTP 기반 애플리케이션은 애플리케이션 상태를 확인하는 엔드포인트를 제공합니다. 클러스터는 이 엔드포인트를 관찰하고 애플리케이션의 잠재적 문제를 완화하도록 구성할 수 있습니다.

애플리케이션은 이러한 엔드포인트를 제공해야 합니다. 개발자는 애플리케이션에서 상태를 확인하는 방법을 결정해야 합니다.

예를 들어 애플리케이션이 데이터베이스 연결에 종속된 경우 데이터베이스에 연결할 수 있는 경우에만 애플리케이션이 정상 상태로 응답할 수 있습니다. 그러나 데이터베이스 연결을 설정하는 모든 애플리케이션에 이러한 확인이 필요한 것은 아닙니다. 이 결정은 개발자의 재량에 따릅니다.

== Kubernetes 애플리케이션 안정성
애플리케이션 포드가 충돌하면 요청에 응답할 수 없습니다. 구성에 따라 클러스터는 포드를 자동으로 다시 시작할 수 있습니다. 포드가 충돌하지 않고 애플리케이션이 실패하면 포드는 요청을 수신하지 않습니다. 그러나 클러스터는 적절한 상태 프로브를 통해서만 이 작업을 수행할 수 있습니다.

Kubernetes는 다음 HA 기술을 사용하여 애플리케이션 안정성을 개선합니다.

* 포드 다시 시작: 포드에서 재시작 정책을 구성하면 클러스터가 오작동하는 애플리케이션 인스턴스를 다시 시작합니다.

* 프로빙: 클러스터는 상태 프로브를 사용하여 애플리케이션이 요청에 응답할 수 없는 시기를 파악하고 문제를 완화하기 위해 자동으로 조치를 취할 수 있습니다.

* 수평적 확장: 애플리케이션 로드가 변경되면 클러스터는 로드에 맞게 복제본 수를 확장할 수 있습니다.



= 애플리케이션 상태 프로브

== Kubernetes 프로브
상태 프로브는 강력한 클러스터를 유지 관리하는 데 있어 중요한 부분입니다. 프로브 를 사용하면 클러스터에서 응답을 반복적으로 프로빙하여 애플리케이션의 상태를 확인할 수 있습니다.

상태 프로브 집합은 다음 작업을 수행하는 클러스터의 기능에 영향을 줍니다.

* 실패한 포드를 자동으로 다시 시작하여 충돌 완화

* 정상 포드에만 요청을 전송하여 페일오버 및 부하 분산

* 포드가 실패하는지 여부와 시기를 확인하여 모니터링

* 새 복제본이 요청을 수신할 준비가 된 시점을 확인하여 확장

== 프로브 엔드포인트 작성
애플리케이션 개발자는 애플리케이션 개발 중에 Health Probe 엔드포인트를 코딩해야 합니다. 이러한 엔드포인트는 애플리케이션의 상태를 결정합니다. 예를 들어 데이터 기반 애플리케이션은 데이터베이스에 연결할 수 있는 경우에만 성공적인 Health Probe를 보고할 수 있습니다.

클러스터에서 자주 호출하므로 Health Probe 엔드포인트를 신속하게 수행해야 합니다. 엔드포인트는 복잡한 데이터베이스 쿼리 또는 많은 네트워크 호출을 수행하면 안 됩니다.

== 프로브 유형
Kubernetes는 시작, 준비, 활성 상태의 프로브 유형을 제공합니다. 애플리케이션에 따라 이러한 유형 중 하나 이상을 구성할 수 있습니다.

=== 준비 프로브
준비 프로브 는 애플리케이션이 요청을 처리할 준비가 되었는지 여부를 확인합니다. 준비 프로브가 실패하면 Kubernetes는 서비스 리소스에서 포드의 IP 주소를 제거하여 클라이언트 트래픽이 애플리케이션에 도달하지 못하게 합니다.

준비 프로브는 애플리케이션에 영향을 줄 수 있는 일시적인 문제를 감지하는 데 도움이 됩니다. 예를 들어 애플리케이션은 초기 네트워크 연결을 설정하고, 캐시에 파일을 로드하거나, 완료하는 데 시간이 걸리는 초기 작업을 수행해야 하기 때문에 시작할 때 일시적으로 사용하지 못할 수 있습니다. 애플리케이션은 때때로 클라이언트에서 일시적으로 사용할 수 없는 긴 배치 작업을 실행해야 할 수 있습니다.

Kubernetes는 애플리케이션이 실패한 후에도 프로브를 계속 실행합니다. 프로브가 다시 성공하면 Kubernetes가 포드의 IP 주소를 서비스 리소스에 다시 추가하고 요청이 포드로 다시 전송됩니다.

이러한 경우 준비 프로브는 일시적인 문제를 해결하고 애플리케이션 가용성을 개선합니다.

=== 활성 프로브
준비 프로브와 마찬가지로 활성 프로브 는 애플리케이션 수명 중에 호출됩니다. 활성 프로브는 애플리케이션 컨테이너 상태가 정상인지 확인합니다. 애플리케이션이 활성 프로브에 어느 횟수만큼 실패하면 클러스터는 재시작 정책에 따라 포드를 다시 시작합니다.

시작 프로브와 달리 활성 프로브는 애플리케이션의 초기 시작 프로세스 후에 호출됩니다. 일반적으로 이 완화 방법을 사용하려면 포드를 다시 시작하거나 다시 생성해야 합니다.

=== 시작 프로브
시작 프로브 는 애플리케이션 시작이 완료되는 시점을 확인합니다. 활성 프로브와 달리 시작 프로브는 프로브가 성공한 후에 호출되지 않습니다. 구성 가능한 제한 시간 후에 시작 프로브가 성공하지 못하면 restartPolicy 값에 따라 포드가 다시 시작됩니다.

시작 시간이 긴 애플리케이션에 시작 프로브를 추가하는 것이 좋습니다. 시작 프로브를 사용하면 활성 프로브를 짧게 유지하고 응답할 수 있습니다.

== 테스트 유형
프로브를 정의할 때 수행할 테스트 유형 중 하나를 지정해야 합니다.

* *HTTP GET*
프로브가 실행될 때마다 클러스터는 지정된 HTTP 엔드포인트에 요청을 보냅니다. 요청이 200 및 399 사이의 HTTP 응답 코드로 응답하면 테스트가 성공한 것으로 간주됩니다. 다른 응답으로 인해 테스트가 실패합니다.

* *컨테이너 명령*
프로브가 실행될 때마다 클러스터는 컨테이너에서 지정된 명령을 실행합니다. 명령이 0 상태 코드와 함께 종료되면 테스트가 성공한 것입니다. 다른 상태 코드로 인해 테스트가 실패합니다.

* *TCP 소켓*
프로브가 실행될 때마다 클러스터는 컨테이너에 대한 소켓을 열려고 시도합니다. 연결이 설정된 경우에만 테스트가 성공합니다.

* *타이밍 및 임계값*
모든 유형의 프로브에는 타이밍 변수가 포함됩니다. period seconds 변수는 프로브가 실행되는 빈도를 정의합니다. failure threshold 는 프로브 자체가 실패하기 전에 필요한 실패한 시도 횟수를 정의합니다.

예를 들어 실패 임계값이 3 이고 period seconds가 5 인 프로브는 전체 프로브가 실패하기 전에 최대 세 번 실패할 수 있습니다. 이 프로브 구성을 사용하면 문제가 완화되기 전에 10초 동안 문제가 존재할 수 있습니다. 그러나 프로브를 너무 자주 실행하면 리소스가 낭비될 수 있습니다. 



= Liveness & Readiness

네임스페이스를 생성 후, 올바른 네임스페이스에 있는지 확인하세요.


[#kubectl-deploy-app]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc new-project probe-%userid%
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
namespace/probe-%userid% created
----

NOTE: `oc new-project probe-%userid%` : probe-%userid%라는 새 프로젝트(네임스페이스)를 생성합니다.

[#kubectl-deploy-app]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc project probe-%userid%
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Now using project "probe-%userid%" on server "https://172.30.0.1:443".
----

NOTE: `oc project probe-%userid%` : 현재 활성화된 컨텍스트의 기본 네임스페이스를 probe-%userid%로 변경합니다.



네임스페이스에서 아무것도 실행되고 있지 않은지 확인하세요.

[#no-resources-resource]
[.console-input]
[source, bash]
----
oc get all
----

[.console-output]
[source,bash]
----
No resources found in myspace namespace.
----











이제 Liveness 및 Readiness 프로브 세트를 사용하여 애플리케이션을 배포하겠습니다.  아래의 배포 yaml을 살펴보세요.


[.console-output]
[source,yaml]
.{quick-open-file}
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
        env: dev
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
              port: 8080
              path: /alive
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 2
        readinessProbe:
          httpGet:  
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 3
----

NOTE: `livenessProbe:` 와 `readinessProbe:` 섹션이 추가 된 것을 확인할 수 있습니다.


이제 다음 명령을 사용하여 이 배포를 적용합니다.

[#create-app-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
        env: dev
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
              port: 8080
              path: /alive
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 2
        readinessProbe:
          httpGet:  
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 3
EOF
----



deployment의 정보를 확인합니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc describe deployment myboot
----


[.console-output]
[source.bash]
----
...
    Image:      quay.io/rhdevelopers/myboot:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     1
      memory:  400Mi
    Requests:
      cpu:        250m
      memory:     300Mi
    Liveness:     http-get http://:8080/ delay=10s timeout=2s period=5s #success=1 #failure=3
    Readiness:    http-get http://:8080/health delay=10s timeout=1s period=3s #success=1 #failure=3
...    
----



Service를 배포하세요:

[#deploy-myboot-rolling]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: v1
kind: Service
metadata:
  name: myboot
  labels:
    app: myboot    
spec:
  ports:
  - name: http
    port: 8080
  selector:
    app: myboot
  type: LoadBalancer
EOF
----




replicas 변경:

[#change-replicas]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc scale deployment myboot --replicas=3
----



* *Terminal#2에서 작업*

반복적으로 서비스에 curl을 시도하세요.


[#kubectl-deploy-app]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc project resource-%userid%
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Now using project "resource-%userid%" on server "https://172.30.0.1:443".
----

NOTE: `oc project resource-%userid%` : 현재 활성화된 컨텍스트의 기본 네임스페이스를 resource-%userid%로 변경합니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
IP=$(kubectl get service myboot -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
----


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
PORT=$(kubectl get service myboot -o jsonpath="{.spec.ports[*].port}")
----


Poll the endpoint:

[#poll-endpoint]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
while true
do curl $IP:$PORT
sleep 0.8
done
----


* *Terminal#1에서 작업*


이미지를 변경하세요.

[#change-deployment-v2-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc set image deployment/myboot myboot=quay.io/rhdevelopers/myboot:v2
----

오류 없는 롤링 업데이트를 확인하세요.


* *Terminal#2에서 확인*


[.console-output]
[source.bash]
----
Aloha from Spring Boot! 131 on myboot-845968c6ff-k4rvb
Aloha from Spring Boot! 134 on myboot-845968c6ff-9wvt9
Aloha from Spring Boot! 122 on myboot-845968c6ff-9824z
Bonjour from Spring Boot! 0 on myboot-8449d5468d-m88z4
Bonjour from Spring Boot! 1 on myboot-8449d5468d-m88z4
Aloha from Spring Boot! 135 on myboot-845968c6ff-9wvt9
Aloha from Spring Boot! 133 on myboot-845968c6ff-k4rvb
Aloha from Spring Boot! 137 on myboot-845968c6ff-9wvt9
Bonjour from Spring Boot! 3 on myboot-8449d5468d-m88z4
----

* *Terminal#1에서 작업*

서비스의 일부인 Pod를 확인하려면 엔드포인트를 살펴보세요.

[#get-endpoints-before]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get endpoints myboot -o json | jq '.subsets[].addresses[].ip'
----

준비 상태 프로브를 통과한 Pod IP는 다음과 같습니다.

[.console-output]
[source.bash]
----
"10.129.2.40"
"10.130.2.37"
"10.130.2.38"
----




=== Readiness Probe

단일 Pod를 'exec'옵션으로 실행하고 준비 상태 플래그를 변경합니다.

[#misbehave-app-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get pod
----

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-845968c6ff-9wshg   1/1     Running   0          11m
myboot-845968c6ff-k5lcb   1/1     Running   0          12m
myboot-845968c6ff-zsgx2   1/1     Running   0          11m
----

[#misbehave-app-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc exec -it myboot-845968c6ff-k5lcb /bin/bash
----

IMPORTANT: 명령어의 pod name(myboot-845968c6ff-k5lcbg 부분)은 실제 조회된 pod의 값으로 변경해야 합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
curl localhost:8080/misbehave
exit
----


NOTE: 명령어는 해당 Pod의 readiness probe에 더이상 정상적으로 응답할 수 없도록 만듭니다.


해당 Pod가 준비 상태가 되지 못하는 것을 확인합니다.

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-845968c6ff-9wshg   1/1     Running   0          11m
myboot-845968c6ff-k5lcb   0/1     Running   0          12m
myboot-845968c6ff-zsgx2   1/1     Running   0          11m
----

이제 엔드포인트를 확인하세요.

[#get-endpoints-after]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get endpoints myboot -o json | jq '.subsets[].addresses[].ip'
----

이제 해당 포드가 서비스의 로드 밸런서에서 누락되었습니다.

[.console-output]
[source.bash]
----
"10.130.2.37"
"10.130.2.38"
----



=== Liveness Probe

deployment의 이미지를 변경합니다.

[#change-deployment-v3-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc set image deployment/myboot myboot=quay.io/rhdevelopers/myboot:v3
----

3개 복제본 모두에서 롤아웃이 완료될 때까지 기다립니다.

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-56659c9d69-6sglj   1/1     Running   0          2m2s
myboot-56659c9d69-mdllq   1/1     Running   0          97s
myboot-56659c9d69-zjt6q   1/1     Running   0          72s
----

 curl loop/poller에서 이미지 변경에 따른 변화를 볼 수 있습니다.:




[.console-output]
[source.bash]
----
Jambo from Spring Boot! 40 on myboot-56659c9d69-mdllq
Jambo from Spring Boot! 26 on myboot-56659c9d69-zjt6q
Jambo from Spring Boot! 71 on myboot-56659c9d69-6sglj
----


[.console-input]
[source,bash]
----
oc get pods
----

[.console-output]
[source,bash]
----
NAME                      READY   STATUS        RESTARTS   AGE
myboot-558b4f8678-nw762   1/1     Running       0          59s
myboot-558b4f8678-qbrgc   1/1     Running       0          81s
myboot-558b4f8678-z7f9n   1/1     Running       0          36s
----

이제 포드 중 하나를 선택하고 'exec'로 실행합니다.

[#shot-v3-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc exec -it myboot-558b4f8678-qbrgc /bin/bash
----

IMPORTANT: 명령어의 pod name(myboot-558b4f8678-qbrgc 부분)은 실제 조회된 pod의 값으로 변경해야 합니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
curl localhost:8080/shot
exit
----

NOTE: 해당 명령어는 Liveness probe에 정상적인 응답을 할 수 없도록 조치합니다.

그리고 livenessProbe의 실패로 인해 Pod가 다시 시작되는 것을 볼 수 있습니다:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch oc get pod
----


[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-558b4f8678-nw762   1/1     Running   0          4m7s
myboot-558b4f8678-qbrgc   1/1     Running   1          4m29s
myboot-558b4f8678-z7f9n   1/1     Running   0          3m44s
----


NOTE: 해당 Pod가 재시작되어 "RESTARTS" 카운트가 1로 변경된 것을 확인할 수 있습니다.




==== Clean up

[#cleanup-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc delete deployment myboot
----










=== Startup Probe

일부 응용 프로그램은 처음 초기화할 때 추가 시작 시간이 필요합니다.

실행 시간 동안 이상을 감지하고 긴 시작 시간을 처리하기 위해 정상적인 동작을 구성해야 하기 때문에 이 시나리오를 활성/준비 프로브에 적용하는 것은 까다로울 수 있습니다.


예를 들어, 교착 상태에 빠질 수 있는 애플리케이션이 있고 이러한 문제를 즉시 파악하고 싶다면 짧은 응답시간의 활성 및 준비 상태 프로브가 있을 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
        env: dev
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
              port: 8080
              path: /alive
          periodSeconds: 1
          timeoutSeconds: 1
          failureThreshold: 1
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 1
----

NOTE: 해당 probe 설정은 timeout 시간이 매우 짧은 것을 볼 수 있습니다.


 해당 배포를 적용합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
        env: dev
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
              port: 8080
              path: /alive
          periodSeconds: 1
          timeoutSeconds: 1
          failureThreshold: 1
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 1
EOF
----

포드 감시에서 볼 수 있듯이 포드는 계속해서 다시 시작되며, 때로는 성공적으로 부팅된 후에도(kubelet이 다시 시작하도록 예약하기 때문에) 이는 SpringBoot의 시작 시간 때문입니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get pod
----


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc describe pods
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  96s                 default-scheduler  Successfully assigned myspace/myboot-849ccd6948-8vrfq to devnation
  Normal   Pulled     92s                 kubelet            Successfully pulled image "quay.io/rhdevelopers/myboot:v1" in 3.295180194s
  Normal   Created    55s (x2 over 92s)   kubelet            Created container myboot
  Normal   Started    55s (x2 over 92s)   kubelet            Started container myboot
  Normal   Pulled     55s                 kubelet            Successfully pulled image "quay.io/rhdevelopers/myboot:v1" in 3.289395484s
  Warning  Unhealthy  52s (x4 over 90s)   kubelet            Liveness probe failed: Get "http://172.17.0.4:8080/alive": dial tcp 172.17.0.4:8080: connect: connection refused
  Normal   Killing    52s (x2 over 88s)   kubelet            Container myboot failed liveness probe, will be restarted
  Normal   Pulling    22s (x3 over 95s)   kubelet            Pulling image "quay.io/rhdevelopers/myboot:v1"
  Warning  Unhealthy  19s (x10 over 88s)  kubelet            Readiness probe failed: Get "http://172.17.0.4:8080/health": dial tcp 172.17.0.4:8080: connect: connection refused
----

*startup probe*는 이 문제를 해결합니다. 시작 프로브가 성공하면 나머지 프로브가 인계받습니다. 그러나 시작 프로브가 통과할 때까지는 활성 상태 프로브나 준비 프로브가 모두 실행될 수 없습니다.



이 섹션에서 차이점을 확인할 수 있습니다.

[.console-output]
[source,yaml]
----
        startupProbe:
          httpGet:
            path: /alive
            port: 8080
          failureThreshold: 6
          periodSeconds: 5
          timeoutSeconds: 1
----






그런 다음 해당 배포를 적용합니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
        env: dev
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
              port: 8080
              path: /alive
          periodSeconds: 1
          timeoutSeconds: 1
          failureThreshold: 1
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 1
        startupProbe:
          httpGet:
            path: /alive
            port: 8080
          failureThreshold: 6
          periodSeconds: 5
          timeoutSeconds: 1
EOF
----



시작 프로브는 애플리케이션을 시작하기 위해 30초(`5 * 6`) 동안 기다립니다.  또한 활성 상태 및 준비 상태 확인 지연 시간이 0으로 낮아졌습니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch oc get pods
----

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-579cc5cc47-2bk5p   0/1     Running   0          67s
----

결국 컬 루프에는 포드가 실행 중인 것으로 표시되어야 합니다.

----
Aloha from Spring Boot! 18 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 19 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 20 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 21 on myboot-849ccd6948-8vrfq
----


==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc delete deployment myboot
oc delete svc myboot
----

